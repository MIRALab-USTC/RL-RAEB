
In many real-world applications of reinforcement learning (RL), performing actions requires consuming certain types of resources that are non-replenishable in each episode. Typical applications include autonomous driving with limited fuel and video games with consumable items. Moreover, the resources can be scarce. In tasks with non-replenishable and scarce resources, we observe that popular RL methods, such as proximal policy optimization and soft actor critic, suffer from high sample complexity, as they tend to exhaust resources fast and thus the subsequent exploration is restricted due to the absence of resources. To address this challenge, we first formalize the aforementioned problem as resource-restricted reinforcement learning, and then propose a novel resource-aware exploration bonus (RAEB) to make reasonable usage of resources. An appealing feature of RAEB is that, it can significantly reduces unnecessary resource-consuming trials, while effectively encouraging the agent to explore unvisited states. Experiments demonstrate that the proposed RAEB significantly outperforms state-of-the-art exploration strategies in resource-restricted RL tasks, improving the sample efficiency by up to an order of magnitude.